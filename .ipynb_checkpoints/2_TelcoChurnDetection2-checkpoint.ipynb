{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection du churn - opérateur Télécom\n",
    "## _Partie 2_\n",
    "\n",
    "## Objectif\n",
    "Dans la première partie, nous avons appris les notions de classification binaires et avons entraîné un premier modèle.\n",
    "\n",
    "Dans cette seconde partie, nous nous attarderons un peu plus sur la validation du modèle et l'analyse de ses performances. Nous verrons 2 notions fondamentales que sont le sur-apprentissage (*overfitting*) et le sous-apprentissage (*underfitting*).\n",
    "\n",
    "#### Les sujets abordés : \n",
    "\n",
    "* Séparation *trainset* / *testset*\n",
    "* Sur-apprentissage / sous-apprentissage\n",
    "* k-fold cross-validation \n",
    "* Hyper-paramètres d'un modèles\n",
    "* Hyper-optimisation\n",
    "\n",
    "#### Contexte\n",
    "Voir partie 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "Pour cette partie, on reprend simplement le code du notebook précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used below\n",
    "def transform_to_binary(val):\n",
    "    \"\"\"\n",
    "    val is equal to \"Yes\" or \"No\"\n",
    "    This function should return 0 or 1\n",
    "    \"\"\"\n",
    "    return int(val == \"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Partner</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Churn</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>24.10</td>\n",
       "      <td>1734.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>88.15</td>\n",
       "      <td>3973.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>74.95</td>\n",
       "      <td>2869.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>55.90</td>\n",
       "      <td>238.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>53.45</td>\n",
       "      <td>119.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>95.00</td>\n",
       "      <td>95.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>91.10</td>\n",
       "      <td>2198.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>21.15</td>\n",
       "      <td>306.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>99.45</td>\n",
       "      <td>1200.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>19.80</td>\n",
       "      <td>457.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5986 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dependents  Partner  SeniorCitizen  Churn  tenure  MonthlyCharges  \\\n",
       "1869           1        1              0      0      72           24.10   \n",
       "4528           0        0              0      0      44           88.15   \n",
       "6344           0        1              0      1      38           74.95   \n",
       "6739           0        0              0      0       4           55.90   \n",
       "432            0        0              0      0       2           53.45   \n",
       "...          ...      ...            ...    ...     ...             ...   \n",
       "3772           0        1              0      1       1           95.00   \n",
       "5191           1        1              0      0      23           91.10   \n",
       "5226           1        1              0      0      12           21.15   \n",
       "5390           0        0              0      1      12           99.45   \n",
       "860            0        0              0      0      26           19.80   \n",
       "\n",
       "      TotalCharges  \n",
       "1869       1734.65  \n",
       "4528       3973.20  \n",
       "6344       2869.85  \n",
       "6739        238.50  \n",
       "432         119.50  \n",
       "...            ...  \n",
       "3772         95.00  \n",
       "5191       2198.30  \n",
       "5226        306.05  \n",
       "5390       1200.15  \n",
       "860         457.30  \n",
       "\n",
       "[5986 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change here if needed\n",
    "dataset_path = \"./datasets/TelcoChurnDetection/telecom_users.csv\"\n",
    "\n",
    "# Read the data\n",
    "dataset = pd.read_csv(dataset_path, index_col=0)\n",
    "\n",
    "# features\n",
    "categorical_features = [\"Dependents\", \"Partner\", \"SeniorCitizen\", 'Churn']\n",
    "numerical_features = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "dataset = dataset[categorical_features+numerical_features].copy()\n",
    "\n",
    "# Converting\n",
    "dataset.TotalCharges = pd.to_numeric(dataset.TotalCharges, errors='coerce').fillna(0)\n",
    "for feature in categorical_features:\n",
    "    dataset[feature] = dataset[feature].apply(transform_to_binary)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation train / test\n",
    "\n",
    "Dans le cadre de votre mission, pour élabore un modèle, vous explorez et analyser des données dites \"froides\" : elles ont été extraites de bases de données à un moment données et ne sont pas mises à jour.\n",
    "\n",
    "Comment garantir que la précision et le rappel que vous avez mesurés soient identiques sur des données plus récentes ?\n",
    "\n",
    "Est-ce que le modèle a réellement _appris_ à détecter des signaux de churn, ou est-ce qu'il a appris à partir d'autres motifs, \"_par chance_\" ?\n",
    "\n",
    "Afin de répondre à ces questions, la méthode consiste à séparer le dataset en 2 :\n",
    "* le trainset : c'est le jeu d'entraînement, sur lequel l'algorithme va apprendre à détecter le churn\n",
    "* le testset : c'est un jeu de test. **Ce jeu n'est pas utilisé pour l'entraînement** mais seulement pour valider les résultats. \n",
    "\n",
    "Dans les lignes suivantes, nous allons séparer le dataset en 2 parties puis effectuer cette démarche de validation.\n",
    "\n",
    "#### Remarque générale : pensez à bien mettre votre code en fonction afin de pouvoir le réutiliser à la fois pour le traiset et le testset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = dataset.drop([\"Churn\"], axis=1).copy()\n",
    "y = dataset[\"Churn\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random of split of the dataset : 70/30\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"Shape of 'X_train' is: {X_train.shape}\")\n",
    "print(f\"Shape of 'X_test' is: {X_test.shape}\")\n",
    "print(f\"Shape of 'y_train' is: {y_train.shape}\")\n",
    "print(f\"Shape of 'y_test' is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show proportion of target in each set\n",
    "churn_rate_train = ???\n",
    "churn_rate_test = ???\n",
    "\n",
    "print(f\"Churn Rate in trainset is: {churn_rate_train:.3f}\")\n",
    "print(f\"Churn Rate in testset is: {churn_rate_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, train a DecisionTree\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to compute prediction on both trainset and testset\n",
    "y_train_pred = ???\n",
    "y_test_pred = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix both on train and test\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for lab, cm in zip([\"TRAINSET\", \"TESTSET\"], [mat_train, mat_test]):\n",
    "    fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "    sns.heatmap(cm, \n",
    "                xticklabels=['Not Churn', 'Churn'],\n",
    "                yticklabels=['Not Churn', 'Churn'],\n",
    "                annot=True,ax=ax1,fmt=\"d\",\n",
    "                linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "    plt.title(f'Confusion Matrix {lab}', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to plot the ROC curve, compute predict socre\n",
    "y_train_pred = ???\n",
    "y_test_pred = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot here the ROC Curve\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarques\n",
    "\n",
    "L'entraînement avec un arbre de décision simple a tendance à apprendre \"trop\" sur le jeu d'entraînement, mais les performances sont bien moins bonnes sur le jeu de données de test.\n",
    "\n",
    "On dit qu'il a _overfitté_ notre dataset.\n",
    "\n",
    "Pour corriger le problème, on va réduire la complexité du modèle (le nombre paramètres) afin d'obtenir un modèle plus robuste. C'est la capacité de **généralisation** du modèle.\n",
    "\n",
    "Pour la suite, nous utiliserons des Random Forest et nous essayerons de trouver la profondeur optimale des arbres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_randomForest_auc(max_depths):\n",
    "    \"\"\"\n",
    "    This function compute the ROC AUC of a RandomForest algorithm\n",
    "    Input: max_depths : paramter to pass to the algorithm\n",
    "    Output\n",
    "    * roc auc on traintest\n",
    "    * roc auc on testset\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "compute_randomForest_auc(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(range(3,11)) +[15, 20, 30]\n",
    "train_scores, test_scores = zip(*[compute_randomForest_auc(n) for n in xx])\n",
    "\n",
    "plt.plot(xx, train_scores, label=\"TRAIN Scores\")\n",
    "plt.plot(xx, test_scores, label=\"TEST Scores\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que plus les arbres sont grands, plus l'écart entre traintest & testset est grande et les modèles vont 'surapprendre\".\n",
    "\n",
    "\n",
    "Un des grands intérêt de l'algorithme RandomForest est qu'il est très robuste au sur-entraînement. En particulier, il est possible de jouer sur différents paramètres comme le nombre d'arbres, et sur des paramètres rendant l'algorithme plus ou moins aléatoires (comme : la quantité de features ou d'observations prises en compte lors de la construction de chaque arbre.)\n",
    "\n",
    "On appelle ces valeurs des **hyper-paramètres** du modèle (les **paramètres** d'un modèle étant le choix des feautures et les valeurs de seuils dans chaque noeud de l'arbre). Les **hyper-paramètres** contrôle comment l'algorithme apprend à partir des données, c'est à dire comment il génère les **paramètres** du modèle.\n",
    "\n",
    "dans les cellules suivantes, analyser l'importance des hyper-paramètres suivant : \n",
    "* n_estimators\n",
    "* min_samples_split\n",
    "* max_features  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross-validation\n",
    "\n",
    "\n",
    "La validation croisée 'K-Fold' est une technique pour analyser la capacité de **généralisation** d'un modèle, autrement dit sa **variance**.\n",
    "\n",
    "Dans la section précédente, on a séparé traintest et testset.\n",
    "\n",
    "Avec la validation croisée K-Fold, le principale est le suivant. Prenons K = 5. On sépare le dataset complet en 5 partie égale.\n",
    "Alors pour chacune de ces 5 parties (\"Fold\") : \n",
    "* l'entraînement est effectué sur les 4 autres (soit 80% du dataset)\n",
    "* la validation sur la 5e partie (soit 20% du dataset)\n",
    "\n",
    "Ce qui fait 5 entraînements / validations.\n",
    "\n",
    "\n",
    "Cette technique est particulièrement importante si le dataset n'est pas très grand et on suspecte un risque de forte variation entre les entraînements.\n",
    "\n",
    "Le but de la cellule suivante est d'utiliser cette technique sur notre jeu de données précédent.\n",
    "\n",
    "#### Que pouvez-vous dire de la stabilité de l'entraînement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we write a function to help implement K-fold validation\n",
    "def compute_RF_performance(X_train, y_train, X_test, y_test, **params):\n",
    "    \"\"\"\n",
    "    This function computes performance of RF for given Train/Test set\n",
    "    X_train/y_train are features and target for trainset\n",
    "    X_test/y_test are features and target for testset\n",
    "    params is a set of parameters to pass to the algorithm\n",
    "    It should plot the ROC Curve\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "compute_RF_performance(X_train, y_train, X_test, y_test, n_estimators=50, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here do the actual K-Fold\n",
    "# Hint : check out doc for dataframe method iloc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=5) \n",
    "for train, test in cv.split(X):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-optimisation du modèle\n",
    "\n",
    "Dans les dernières sections nous avons vu l'influence des **hyper-paramètres** de l'algorithme sur la performance du modèle.\n",
    "\n",
    "Naturellement, nous souhaitons optimiser le choix de ces hyper-paramètres afin d'avoir le meilleur modèle possible.\n",
    "\n",
    "Il existe plusieurs stratégie. La plus simple : la stratégie du '_GridSearch_' consiste à rechercher de manière brutale toutes les combinaisons possibles pour sélectionner la meilleure.\n",
    "\n",
    "Comme ce processus est long, on propose une seconde approche qui consiste à tester \"aléatoirement\" parmi un ensemnble de valeurs possibles.\n",
    "\n",
    "D'autres stratégies plus élaborée existent. Pour toutes ces stratégies, il est nécessaire de définir :\n",
    "* ce qu'on veut optimiser. Par exemple la moyenne 4-fold de la précision (accuracy)\n",
    "* définir un espace de recherche des hyper paramètres (_paramters space_)\n",
    "\n",
    "\n",
    "C'est ce que nous allons implémenter dans les prochaines lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify parameters spaces\n",
    "\n",
    "distributions = dict(\n",
    "    n_estimators = [50, 100, 200, 500, 1000],\n",
    "    max_depth = [3, 5, 8, 10, 15]\n",
    "    # TODO : add here other parameters you want to test\n",
    ")\n",
    "\n",
    "print(f\"Size of parameters space is: {np.prod([len(v) for v in distributions.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the RandomizedSearch with Cross-Vald optimisation\n",
    "# The following function from scikit learn implement both randomized search and K-Fold.\n",
    "# By default, it uses the scores function of the given classifier, which is the accuracy for RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = RandomForestClassifier(<add here constant parameters>)\n",
    "n_iter = 20\n",
    "\n",
    "optimized_clf = ???\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_iter), optimized_clf.cv_results_['mean_test_score'])\n",
    "plt.plot(range(n_iter), optimized_clf.cv_results_['mean_test_score'] + optimized_clf.cv_results_['std_test_score'])\n",
    "plt.plot(range(n_iter), optimized_clf.cv_results_['mean_test_score'] - optimized_clf.cv_results_['std_test_score'])\n",
    "plt.title(\"Successive test score (mean +/- std)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tout au long de ce notebook, nous avons pu observer comment les performances d'un algorithme peuvent varier en fonction de l'entraînement et des hyper-paramètres.\n",
    "\n",
    "L'évaluation d'un modèle doit être très rigoureuse ! En effet, il est nécessaire de constamment se poser la question de la qualité de l'apprentissage du modèle : est-ce le modèle est en sur-apprentissage ? est-ce que le modèle a suffisamment appris ?\n",
    "Les meilleurs modèles sont ceux qui parviennent à réaliser au mieux ce compromis entre biais et variance, autrement dit entre un modèle riche et une capacité de généralisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
